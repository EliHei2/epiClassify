{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from synthetic import generate_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1000\n",
    "n_features = 100\n",
    "n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,graph = generate_dataset(n_classes,n_obs,n_features,nb_characteristic_features=3, \\\n",
    "                             signal=2,diffusion_coefficient=0.2,noise=0)\n",
    "\n",
    "tensor_x = torch.Tensor(X).float()\n",
    "tensor_y = torch.Tensor(y).long()\n",
    "\n",
    "dataset = TensorDataset(tensor_x,tensor_y) \n",
    "train_dataset,test_dataset = random_split(dataset,[int(len(dataset)*0.7),int(len(dataset)*0.3)])\n",
    "train_dataloader = DataLoader(train_dataset,shuffle=True,batch_size=10) \n",
    "test_dataloader = DataLoader(test_dataset,shuffle=True,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,n_features,n_layers,n_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(nn.Linear(n_features,n_features))\n",
    "        self.last_layer = nn.Linear(n_features,n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP(n_features,2,n_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 92.973\n",
      "[2] loss: 92.909\n",
      "[3] loss: 92.819\n",
      "[4] loss: 92.754\n",
      "[5] loss: 92.639\n",
      "[6] loss: 92.602\n",
      "[7] loss: 92.499\n",
      "[8] loss: 92.416\n",
      "[9] loss: 92.331\n",
      "[10] loss: 92.247\n",
      "[11] loss: 92.191\n",
      "[12] loss: 92.099\n",
      "[13] loss: 92.031\n",
      "[14] loss: 91.949\n",
      "[15] loss: 91.856\n",
      "[16] loss: 91.800\n",
      "[17] loss: 91.714\n",
      "[18] loss: 91.666\n",
      "[19] loss: 91.585\n",
      "[20] loss: 91.487\n",
      "[21] loss: 91.420\n",
      "[22] loss: 91.354\n",
      "[23] loss: 91.275\n",
      "[24] loss: 91.211\n",
      "[25] loss: 91.117\n",
      "[26] loss: 91.073\n",
      "[27] loss: 91.002\n",
      "[28] loss: 90.925\n",
      "[29] loss: 90.864\n",
      "[30] loss: 90.776\n",
      "[31] loss: 90.718\n",
      "[32] loss: 90.652\n",
      "[33] loss: 90.588\n",
      "[34] loss: 90.539\n",
      "[35] loss: 90.436\n",
      "[36] loss: 90.373\n",
      "[37] loss: 90.313\n",
      "[38] loss: 90.269\n",
      "[39] loss: 90.201\n",
      "[40] loss: 90.118\n",
      "[41] loss: 90.063\n",
      "[42] loss: 89.979\n",
      "[43] loss: 89.921\n",
      "[44] loss: 89.863\n",
      "[45] loss: 89.831\n",
      "[46] loss: 89.766\n",
      "[47] loss: 89.672\n",
      "[48] loss: 89.622\n",
      "[49] loss: 89.553\n",
      "[50] loss: 89.512\n",
      "[51] loss: 89.433\n",
      "[52] loss: 89.381\n",
      "[53] loss: 89.329\n",
      "[54] loss: 89.248\n",
      "[55] loss: 89.212\n",
      "[56] loss: 89.148\n",
      "[57] loss: 89.093\n",
      "[58] loss: 89.036\n",
      "[59] loss: 88.971\n",
      "[60] loss: 88.919\n",
      "[61] loss: 88.861\n",
      "[62] loss: 88.805\n",
      "[63] loss: 88.757\n",
      "[64] loss: 88.708\n",
      "[65] loss: 88.650\n",
      "[66] loss: 88.574\n",
      "[67] loss: 88.535\n",
      "[68] loss: 88.476\n",
      "[69] loss: 88.425\n",
      "[70] loss: 88.388\n",
      "[71] loss: 88.326\n",
      "[72] loss: 88.270\n",
      "[73] loss: 88.213\n",
      "[74] loss: 88.160\n",
      "[75] loss: 88.096\n",
      "[76] loss: 88.066\n",
      "[77] loss: 88.001\n",
      "[78] loss: 87.967\n",
      "[79] loss: 87.957\n",
      "[80] loss: 87.854\n",
      "[81] loss: 87.789\n",
      "[82] loss: 87.743\n",
      "[83] loss: 87.701\n",
      "[84] loss: 87.664\n",
      "[85] loss: 87.594\n",
      "[86] loss: 87.557\n",
      "[87] loss: 87.510\n",
      "[88] loss: 87.462\n",
      "[89] loss: 87.419\n",
      "[90] loss: 87.366\n",
      "[91] loss: 87.319\n",
      "[92] loss: 87.270\n",
      "[93] loss: 87.221\n",
      "[94] loss: 87.185\n",
      "[95] loss: 87.146\n",
      "[96] loss: 87.078\n",
      "[97] loss: 87.019\n",
      "[98] loss: 86.998\n",
      "[99] loss: 86.926\n",
      "[100] loss: 86.887\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 81 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        X, labels = data\n",
    "        outputs = net(X)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
