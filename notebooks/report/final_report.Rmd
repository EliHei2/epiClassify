---
title: "Structural Inductive Bias in Supervised Learning for Single-cell Data"
shorttitle: "STA426-2020 Project Report"
author:
- name: Elyas Heidari
  affiliation:
  - Department of Biological Systems Sciences and Engineering, ETH Zürich, Switzerland
  email:
  - eheidari@student.ethz.ch 
   
date: '`r format(Sys.Date(), "%B %d, %Y")`'
abstract: >
    Recent advancements in single-cell RNA sequencing (scRNAseq) have unraveled the transcriptional diversities underlying heterogeneous batches of cells. Current single-cell classification methods take genes as independent features to assign cell types disregarding their interactome which is of great importance in cellular processes. As a result, existing methods are prone to dataset-specific classification artifacts which prevents generalization of the model for cell-type classification in new datasets. Here, we introduce scPotter which takes feature interactions (e.g., gene-gene interactions) into account to classify single-cells and interpret the marker genes. Using two different peripheral blood mononuclear cell (PBMC) datasets as train and test sets, we demonstrate the potentials of scPotter for transfer learning on single-cell RNA sequencing (scRNAseq) data.
    
output:
    BiocStyle::pdf_document
geometry: "left=10cm,right=0cm,top=0cm,bottom=0cm"
bibliography: references.bib
link-citations: yes
---


```{r setup, include = FALSE}
library('BiocStyle')
knitr::opts_chunk$set(autodep=TRUE, cache=TRUE, dev='png', cache.lazy = FALSE)
# wflow_build(files='analysis/0-preprocessing.Rmd', view=F, verbose=T, delete_cache=F, cache.lazy = FALSE)


```

<!--## Background and motivation
<!-- # General setting, objectives, and challenges -->
# Introduction and motivation
Single-cell transcriptomics has enabled the resolution of transcriptional diversities of individual cells, hence the possibility to classify cells’ identity and function via their transcriptional state. Noting that such identity classifications can elucidate the organization and function of tissues in health and disease states, recently more investments have been devoted to this task, for example, in the Human Cell Atlas project [@hca]. 

Current single-cell RNA sequencing (scRNA-seq) classification methods use gene expression as independent features to assign cell types, disregarding their interactome which is of great importance in the consistency of different cell states. Cell type classification based on a few marker genes, for example, yields better results in intra data set settings where both train and test datasets are sampled from the same dataset [@abdelaal2019comparison] and may fail to correctly classify cell types under perturbations when the expression of the predefined marker genes varies concerning the control reference data set. Moreover, using gene expression as independent features can delude cell lineage relationships for cell states and cell types with a subtle difference in their gene expression profiles. In Fig. \@ref(fig:schema) we schematically illustrate how utilizing gene interactions may provide improvements in cell population deconvolution.


```{r schema, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap='Incorporating feature (gene) Interactions. While using the contemporaneity Euclidean metric disregarding gene interactions on the single cell space could lead to spurious results, incorporating gene interactions can be used to decouple cell populations.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/schema.pdf')
``` 


Structures are ubiquitous in biology. In reality, components of a biological system, interact with each other, forming functional modules (e.g., pathways), and in turn, modules interact to form a complex network creating the biological system [@networks]. Gene regulatory networks, namely, perfectly embody such a phenomenon. While in many applications, as mentioned above, biological states (e.g., cell type), can be described by a few components (e.g., marker genes), incorporating the underlying structure can provide many useful inputs both for discriminative tasks (e.g., cell type classification) and interpretation tasks (e.g., finding marker genes/pathways). This is the goal of this project. That is to leverage structures on which input features function as an inductive bias for supervised learning on single-cell data. More precisely, here we show how one can exploit such underlying structures to perform cell-type classification based on single-cell RNA seq data. 

In this project, we show how including such structures in the neural network classifiers facilitates better accuracy, robustness, and interpretability. We also provide an end-to-end pipeline to reconstruct and feed in such structures to the neural network classifiers. We decompose the problem of supervised learning into two parts. The first part is to train a model to perform annotation, which hereafter we call the "forward" problem. Once the model is trained, we interpret the model, that is, we aim to capture features that are the classes' characteristics\ In other words, we seek a few features which can discriminate classes. We refer to the latter as the "backward" problem.


# Materials and Methods

## PBMC dataset
A pivotal application of cell-type annotation is transferring knowledge gained from one dataset to new unseen datasets. Therefore, we sought fro two datasets with the same cell type profile but acquired in separate experiments to inspect how well the method can transfer between various environments and overcome unwanted variations (such as extrinsic noise and batch effects). We chose two human peripheral blood mononuclear cell (PBMC) scRNA-seq experiments [@ding2019systematic]. The dataset contains two separate experiments, hereafter called PBMC 1 and PBMC 2, generated in a single center, but employing seven different scRNA-seq methods. Both datasets are annotated by cell types which can be used to train and validate our classifiers. This suits our goal to verify our framework on a simulated task of real transfer learning. As in potential applications we expect our methods to learn annotation priorly on the complete (training) dataset and transfer the learned model to annotate the unseen (test) one. 

<!-- ## Models to incorporate genomic locations
Genomic locations and gene proximity on chromosomes can be used as additional information for cell annotation. The approach is inspired by the fact that genes in proximity on the chromosomes are more likely to undergo the same epigenetic events, such as chromatin openness and genomic interactions (e.g., in topologically associating domains, TADs). One can sort genes based on genomic locations. By doing so, each cell can be represented as either a 1-dimensional signal in which each (time) point represents the expression/activation level of the corresponding gene or a linear graph in which each node is assigned by the expression level of the corresponding gene. By such modeling, we use both 1-dimensional Convolutional Neural Networks and Graph Neural Networks (ref). To perform the forward task of supervised learning.  -->

## Models to incorporate gene-gene interactions
It is known that in each cell proteins interact with each other through several regulatory mechanisms, forming an interaction network. In the single-cell field, however, we lack such an information at the protein level, yet, we can use rich RNA-seq datasets. One can use prior information, such as protein-protein interaction databases [@string], and replace genes with their protein products. Also, one can reconstruct the gene-gene interaction network from scratch, by inferring it from the input dataset, e.g., using graphical models. The idea is to provide the model with a structure, on which features (here genes) interact with each other. There is a multitude of approaches to do so, but we do not take all of them into account here. Instead, we introduce a class of models, which apply to all graphical structures.

Recent advancements of deep learning models to capture modular structures in other applications such as image, audio, and speech analysis as well as the emergence of high-throughput scRNA-seq technologies that provide the large amounts of data required for such models motivated us to employ deep learning models applicable to non-regular graph data. Amongst such models, which are generally referred to as "geometric deep learning" (@bronstein2017geometric, @monti2017geometric), Graph Convolutional Networks (GCNs) have significant power in extracting informative feature maps from signals defined over nodes of a graph while considering meaningful local structures and connectivities among them. This makes GCNs suitable for our purpose of analyzing non-regular gene networks [@zhou2018graph].

## Baseline model
We investigate to which extent inferring the graph structure from the data and using it as an inductive bias to a GCN improves robustness, generalization, and interpretation in comparison to the standard fully connected neural networks (FCNNs) without any prior structural information. We use Multi-Layer Perceptrons (MLPs) [@mlp] as the FCNN model.

## Interpreting Neural Network models
While Neural Networks are widely known as a black box, for they are difficult to interpret, several methods have been recently put forward to interpret and explain Neural Networks [@inter]. In particular, captum python library [@captum] has been recently introduced in python to facilitate the interperetibility of neural networks. We use captum to find the characteristic features or marker genes of cell types. All Neural Networks are implemented in python, using pytroch [@pytorch] and pytorch-geometric [@ptg]. For preprocessing, visualization, and exploratory data analysis we used R [@R]. 

# Experiments and results

## Preprocessing 
We take PBMC 1 as our training dataset and PBMC 2 as our test dataset. The task is to learn the model on PBMC 1, and validate it on PBMC 2, by predicting cell type annotations, and comparing them to true labels. Preprocessing includes three steps, first, cell type selection and cell-wise data preprocessing, second, gene subset selection, and lastly, structure reconstruction:

- **Cell type selection & preprocessing:** We select a subset of cell types in the intersection of cell types present in both training and test datasets. Afterward, data for each cell should be preprocessed. Cells with less than 100 counts and cells with a high percentage of mitochondrial gene counts (>0.01) are filtered out due to low sequencing/experimental quality. Finally, we used the scran package [@scran] in R to normalize counts. We ended up with 400 cells per class for the training dataset. We do not subsample the test dataset. Cell counts for training and test datasets are shown in Fig. \@ref(fig:freqs). 

```{r freqs, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='75%', fig.cap='Cell type count profiles of training and test datasets.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/Freqs.pdf')
``` 

- **Gene subset selection:** Following the widely-used procedure in the field, we select highly variable genes, using the scran package. We ended up with 260 genes in total. 

- **Gene network reconstruction:** We use Gaussian graphical models implemented by R package glasso [@glasso] to reconstruct a sparse gene interaction network for the training dataset.  The reconstructed networks utilizing glasso are shown in Fig. \@ref(fig:nets)-A&B. In Fig. \@ref(fig:nets)-A nodes are colored based on their graph communities detected by the Louvain algorithm [@louvain], same colors are used for the second network in Fig. \@ref(fig:nets)-B, reconstructed on the test dataset. For classifiers, we just use the graph reconstructed on the training dataset as we want to leave the test dataset, unseen.

```{r nets, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='100%', fig.cap='The reconstructed gene-interaction networks. Each node represents a gene and each edge represents an interaction, estimated by the glasso algorithm. (A) The interaction network reconstructed on the training dataset, PBMC 1. (B) The interaction network reconstructed on the training dataset, PBMC 2. (C-D) Each node is colored based on the average expression of the gene in Natural Killer Cells, in (C) training and (D) test datasets (the brighter the higher). (E-F) Each node is colored based on the average importance (based on captum) of the gene in Natural Killer Cells, from (E) MLP and (F) TransformerConv classifiers (the brighter the higher).' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/Gene_Networks.pdf')
``` 

A 2-dimensional UMAP embedding of the training and test datasets after preprocessing is shown in Fig. \@ref(fig:maps)-A. As one can see, the classes are not separated and such a heterogeneity affects the classification accuracy. 

```{r umaps, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='100%', fig.cap='UMAP embedding. Left: colored by cell types, Right: colored by scRNAseq technology. (A) Raw data with all 260 genes. (B-C) Just using top 2 gene markers per class (14 total) captured by (B) TransformerConv and (C) MLP classifiers.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/umaps.pdf')
``` 

## Classification based on gene interaction networks
We use a recently-introduced graph neural network, called TransformerConv Net [@TC]. In TransformerConv Net, each node generates a message based on its features and sends it to its neighbors. Then each node aggregates the messages of its neighbors and uses this aggregate to updates its features. The aggregation is done with a permutation-invariant function. In TransformerConv Net, the updated feature vector of node $i$ after one round of message passing is $x_{i}' = W_{1}x_{i} + W_{2}\textrm{mean}_{j\in \mathcal{N}(i)}(x_{j})$ where $W_{1}$ and $W_{2}$ are learnt weight matrices, shared for all of the nodes, and $\mathcal{N}(i)$ is the set of neighbors of $i$. In our setting, each node starts with only one feature (which is the expression level of one gene), but we can have a message-passing function that creates hidden feature vectors of higher dimensions.

As mentioned before, after training the graph neural network, we use captum to find the most important features, based on a well-known interpretation model, the Integrated Gradients method [@ig]. We also compared the results to an MLP which does not impose any structure on the input features.  Both MLP and GCN are optimized naively, by selecting a proper learning rate, dropout, and the number of hidden layers and hidden nodes. The results are shown in Fig. \@ref(fig:preds). The GCN outperforms the MLP on both training and test datasets. By comparison to the Human Protein Atlas [@hpa], our results suggest how using a graphical structure as an inductive bias leads to improvement in interpretation. While the MLP can not fully capture marker genes of the PBMC cell types, GCN perfectly recovers the characteristic genes, the results are coherent with previous results found by relevant toolkits SCANPY (@scanpy, [notebook](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.htmlm)) and Seurat (@seurat, [notebook](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html)) which essentially use statistical methods to find marker genes. Fig. (fig:umaps)-B&C indicate how precise the GCN could deconvolve the cell types (at least at the broad level), and MLP fails to do so. While the MLP simply recovers the genes with the higher mean in each class, GCN recovers the true genes or gene modules that are known to contribute in cell identities (Fig. \@ref(fig:nets)-E&F). 


```{r preds, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='100%', fig.cap='Classification and interpretation results. (A) Prediction accuracies on training and test datasets. (B) Marker genes captured by the GCN model. (C) Marker genes captured by the MLP model.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/Predictions.pdf')
``` 

# Discussion
We developed new tools for leveraging gene interactions in single-cell data analysis. We showed how such a piece of structural information could be employed as an inductive bias for better generalization and interpretation. This project was simply a proof of concept but it sheds light on the importance and utility of interaction networks in the field of single-cell and even more, in other biological applications. The advantage of this approach over similar approaches in contemporary single-cell field is that it can perform both forward and backward tasks of supervised learning in one pipeline, also it is a generic approach that can be well generalized to any biological application, where the task is annotation. 

## Gene network reconstruction and transferability 
We reconstructed gene interaction networks, without any prior knowledge, using the graphical lasso. The graphical lasso leads to sparse yet modular solutions on scRNA-seq data, as shown in Fig. \@ref(fig:nets). This is in line with the fact that the true underlying gene regulatory networks are essentially sparse. Moreover, sparsity reduces the estimation noise and ensures the accuracy of the graph representation. Therefore, imposing such an inductive bias in the GCN framework through the graph structure improves the performance through regularization of the parameter space. One can also compare Fig. \@ref(fig:nets)-A and Fig. \@ref(fig:nets)-B, where the gene-interaction networks are reconstructed on two separate PBMC datasets. Higher compatibility will lead to better transferability of the model along environments. We observe that many gene communities from the PBMC 1 are conversed on the PBMC 2 dataset; suggesting that graphical lasso is a reliable method for reconstructing gene networks with scRNA-seq data, in that, it preserves the global network structures. 



## Software developed
We developed scPotter [GitHub.com/EliHei2/scPotter](https://github.com/EliHei2/scPotter) as an end-to-end pipeline for 1) annotation 2) finding the most important features for each annotation category. The backbone is adopted from two of the packages, previously developed by us, GNNProject [GitHub.com/e-sollier/DL2020](https://github.com/e-sollier/DL2020) in python and scGCNUtiles [GitHub.com/EliHei2/scGCN](https://github.com/EliHei2/scGCN) in R. Many models from synthetic data generation to multiple neural network classifiers are implemented along with tools for visualization and exploratory data analysis. The modular structure of the pipeline enables extensions to new models and analyses. The pipeline is designed to meet state of the art reproducibility standards and criteria of open science, such as coherence, integrity, documentation, and readability.

## Potential applications and future steps
Although here we just focused on gene-gene interaction networks and reconstructed them, the pipeline can be also used on other data modalities (e.g., chromatin structure), where the interaction network is already given. For instance, Hi-C data [@hic] can be used to reconstruct genomic interaction networks, and the rest of the pipeline, that is, classification and interpretation can be used as-is, to find marker genomic locations. 

Genomic locations and gene proximity on chromosomes can be used as additional information for cell annotation. The approach is inspired by the fact that genes in proximity on the chromosomes are more likely to undergo the same epigenetic events, such as chromatin openness and genomic interactions (e.g., in topologically associating domains, TADs). One can sort genes based on genomic locations. By doing so, each cell can be represented as either a 1-dimensional signal in which each (time) point represents the expression/activation level of the corresponding gene or a linear graph in which each node is assigned by the expression level of the corresponding gene. By such modeling, one can use both 1-dimensional Convolutional Neural Networks and Graph Neural Networks. Both of these models are implemented in our pipeline, to perform supervised learning.



\newpage
### Acknowledgments {.unnumbered}
Hereby, I want to appreciate the extremely helpful comments of Will Macnair, Izaskun Mallona, and Stephany Orjuela. I thank my collaborators in the related previous projects, Laleh Haghverdi and Etienne Sollier. Finally, I express my regards to the STA426 instructors, Mark Robinson, Hubert Rehrauer, and Ahmadreza Yousefkhani for motivating me to conduct this piece of research.


### Data availability {.unnumbered}
The raw dataset is available on the Gene Expression Omnibus with accession number [GSE132044](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132044). The preprocessed dataset is available [here](https://polybox.ethz.ch/index.php/s/8cm5WSz70Wq5vmQ).

### Supplementary information {.unnumbered}
The intermediate results and the supplementary information are available on the [project's GitHub repository](https://github.com/EliHei2/scPotter/output).

<!-- # Appendix {.unnumbered}
## Glossary {.unnumbered}
- **Fluency:** in cognitive sciences and its related fields the concept of 'fluency' refers to 'the subjective experience of ease' with which people undergo a cognitive situation, e.g., processing fluency (ease of processing information), perception fluency (ease of perceptual processing), and retrieval fluencey (ease of retrieving information) [@alter2009uniting].
- **Desirable difficulty:** certain difficulties introduced into the learning process which can greatly improve long-term retention of the learned material [@bjork1994memory].
- **Metacognition:** one's knowledge about their cognitive processes, or roughly, 'thinking about thinking' [@flavell1976metacognitive].
- **Judgment of learning:** the assessment one makes about how well they have learned and predictions about how well they will remember the learned information [@son2005judgments]. -->
\newpage
### References {-}