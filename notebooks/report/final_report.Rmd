---
title: "Structural Inductive Bias in Supervised Learning for Single-cell Data"
shorttitle: "STA426-2020 Project Report"
author:
- name: Elyas Heidari
  affiliation:
  - Department of Biological Systems Sciences and Engineering, ETH Zürich, Switzerland
  email:
  - eheidari@student.ethz.ch 
   
date: '`r format(Sys.Date(), "%B %d, %Y")`'
abstract: >
    Recent advancements in single-cell RNA sequencing (scRNAseq) have unraveled the transcriptional diversities underlying heterogeneous batches of cells. Current single-cell classification methods take genes as independent features to assign cell types disregarding their interactome which has been shown to be of great importance in cellular processes. As a result, existing methods are prone to dataset specific classification artifacts which prevents generalization of the model for cell type classification in new datasets. Here, we introduce single-cell Graph Convolutional Network (scGCN) which takes gene-gene interactions into account by constructing a co-expression network of a subset of genes and using a graph deep learning approach to classify single-cells. Using two different peripheral blood mononuclear cell (PBMC) datasets as train and test sets, we demonstrate the potentials of scGCN for transfer learning.
    
output:
    BiocStyle::pdf_document
geometry: "left=10cm,right=0cm,top=0cm,bottom=0cm"
bibliography: references.bib
link-citations: yes
---


```{r setup, include = FALSE}
library('BiocStyle')
knitr::opts_chunk$set(autodep=TRUE, cache=TRUE, dev='png', cache.lazy = FALSE)
# wflow_build(files='analysis/0-preprocessing.Rmd', view=F, verbose=T, delete_cache=F, cache.lazy = FALSE)


```

<!--## Background and motivation
<!-- # General setting, objectives, and challenges -->
# Introduction and motivation
Single cell transcriptomics have enabled resolution of transcriptional diversities of individual cells, hence the possibility to classify cells’ identity and function via their transcriptional state. Noting that such identity classifications can elucidate organization and function of tissues in health and disease states, recently more investments have been devoted to this task, for example, in the Human Cell Atlas project [@hca]. 

Current single-cell RNA sequencing (scRNA-seq) classification methods use genes expression as independent features to assign cell types, disregarding their interactome which is of great importance in consistency of different cell states. Cell type classification based on a few marker genes, for example, yields better results in intra data set settings where both train and test datasets are sampled from the same dataset [@abdelaal2019comparison] and may fail to correctly classify cell types under perturbations when expression of the predefined marker genes varies with respect to the control reference data set. Moreover, using genes expression as independent features can delude cell lineage relationships for cell states and cell types with subtle difference in their gene expression profiles. In Fig. \@ref(fig:schema) we schematically illustrate how utilizing gene interactions may provide improvements in cell population deconvolution.


```{r schema, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap='Incorporating feature (gene) Interactions. While using the contemporaneity Euclidean metric disregarding gene interactions on the single cell space could lead to spurious results, incorporating gene interactions can be used to decouple cell populations.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/schema.pdf')
``` 


Structures are ubiquitous in biology. In reality, components of a biological system, interact with each other, forming functional modules (e.g., pathways), and in turn, modules interact to form a complex network creating the biological system [@networks]. Gene regulatory networks, namely, perfectly embody such a phenomenon. While in many applications, as mentioned above, biological states (e.g., cell type), can be described by a few components (e.g., marker genes), incorporating the underlying structure can provide many useful inputs both for discriminative tasks (e.g., cell type classification) and interpretation tasks (e.g., finding marker genes/pathways). This is the goal of this project. That is to leverage structures on which input features function upon as an inductive bias for supervised learning on single-cell data. More precisely, here we show how one can exploit such underlying structures to perform cell type classification based on single-cell RNA seq data. 

In this project, we show how including such structures in the neural network classifiers facilitates better accuracy, robustness, and interpretablity. We also provide an end-to-end pipeline to reconstruct and feed in such structures to the neural network classifiers. We decompose the problem of supervised learning into two parts. The first part is to train a model to perform annotation, which hereafter we call the "forward" problem. Once the model is trained, we interpret the model, that is, we aim to capture features which are the classes' characteristics\ In other words, we seek for a few number of features which can discriminate classes. We refer to the later as the "backward" problem.


# Materials and Methods

## PBMC dataset
A pivotal application of cell type annotation is transferring knowledge gained from one dataset to new unseen datasets. Therefor, we sought fro two datasets with the same cell type profile, but acquired in separate experiments to inspect how well the method is able to transfer between various environments and overcome unwanted variations (such as extrinsic noise and batch effects). We chose two human peripheral blood mononuclear cell (PBMC) scRNA-seq experiments [@ding2019systematic]. The dataset contains two separate experiments, hereafter called PBMC 1 and PBMC 2, generated in a single center, but employing seven different scRNA-seq methods. Both datasets are annotated by cell types which can be used to train and validate our classifiers. This suits our goal to verify our framework on a simulated task of real transfer learning. As in potential applications we expect our methods to learn annotation priorly on the complete (training) dataset and transfer the learned model to annotate the unseen (test) one. 

<!-- ## Models to incorporate genomic locations
Genomic locations and gene proximity on chromosomes can be used as an additional information for cell annotation. The approach is inspired by the fact that genes in proximity on the chromosomes, are more likely to undergo the same epigenetic events, such as chromatin openness and genomic interactions (e.g., in topologically associating domains, TADs). One can sort genes based on genomic locations. By doing so, each cell can be represented as either a 1-dimensional signal in which each (time) point represents the expression/activation level of the corresponding gene, or a linear graph in which each node is assigned by the expression level of the corresponding gene. By such a modeling, we use both 1-dimensional Convolutional Neural Networks  and Graph Neural Networks (ref). To perform the forward task of supervised learning.  -->

## Models to incorporate gene-gene interactions
It is known that in each cell proteins interact with each other through several regulatory mechanisms, forming an interaction network. In the single-cell field, however, we lack such an information at the protein level, yet, we can use rich RNA-seq datasets. One can use prior information, such as protein-protein interaction databases [@string], and replace genes with their protein products. Also, one can reconstruct the gene-gene interaction network from scratch, by inferring it from the input dataset, e.g., using graphical models. The idea is to provide the model with a structure, on which features (here genes) interact with each other. There is a multitude of approaches to do so, but we do not take all of them into account here. Instead, we introduce a class of models, which are applicable to all graphical structures.

Recent advancements of deep learning models to capture modular structures in other applications such as image, audio and speech analysis as well as emergence of high-throughput scRNA-seq technologies which provide the large amounts of data required for such models motivated us to employ deep learning models applicable to non-regular graph data. Amongst such models, which are generally referred to as "geometric deep learning" (@bronstein2017geometric, @monti2017geometric), Graph Convolutional Networks (GCNs) have significant power in extracting informative feature maps from signals defined over nodes of a graph while considering meaningful local structures and connectivities among them. This makes GCNs suitable for our purpose of analyzing non-regular gene networks [@zhou2018graph].

## Baseline model
We investigate to which extent inferring the graph structure from the data and using it as an inductive bias to a GCN improves robustness, generalization, and interpretation in comparison to the standard fully connected neural networks (FCNNs) without any prior structural information. We use Multi-Layer Perceptrons (MLPs) [@mlp] as the FCNN model.

## Interpreting Neural Network models
While Neural Networks are widely known as black box, for they are difficult to interpret, several methods have been recently put forward to interpret and explain Neural Networks [@inter]. In particular, captum python library [@captum] has been recently introduced in python to facilitate interoperability of neural networks. We use captum to find the characteristic features or marker genes of cell types. All Neural Networks are implement in python, using pytroch [@pytorch] and pytorch-geometric [@ptg]. For preprocessing, visualization, and exploratory data analysis we used R [@R]. 

# Experiments and results

## Preprocessing 
We take PBMC 1 as our training dataset and PBMC 2 as our test dataset. The task is to learn the model on PBMC 1, and validate it on PBMC 2, by predicting cell type annotations, and comparing them to true labels. Preprocessing includes three steps, first, cell type selection and cell-wise data preprocessing, second, gene subset selection and lastly, structure reconstruction:

- **Cell type selection & preprocessing:** We select a subset of cell types in the intersection of cell types present in both training and test datasets. Afterwards, data for each cell should be preprocessed. Cells with less than 100 counts and cells with high percentage of mithocondrial gene counts (>0.01) are filtered out due to low sequencing/experimental quality. Finally, we used scran package [@scran] in R to normalize counts. We ended up with 400 cells per class for the training dataset. We do not subsample the test dataset. Cell counts for training and test datasets is shown in Fig. \@ref(fig:freqs). 

```{r freqs, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='75%', fig.cap='Cell type count profiles of training and test datasets.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/Freqs.pdf')
``` 

- **Gene subset selection:** Following the widely-used procedure in the field, we select highly variable genes, using the scran package. We ended up with 260 genes in total. 

- **Gene network reconstruction:** We use Gaussian graphical models implemented by R package glasso [@glasso] to reconstruct a sparse gene interaction network for the training dataset.  The reconstructed networks utilizing glasso are shown in Fig. \@ref(fig:nets)-A&B. In Fig. \@ref(fig:nets)-A nodes are colored based on their graph communities detected by the Louvain algorithm [@louvain], same colors are used for the second network in Fig. \@ref(fig:nets)-B, reconstructed on the test dataset. For classifiers, we just use the graph reconstructed on the training dataset as we want to leave the test dataset, unseen.

```{r nets, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='100%', fig.cap='The reconstructed gene-interaction networks. Each node represents a gene and each edge represents an interaction, estimated by the glasso algorithm. (A) The interaction network reconstructed on the training dataset, PBMC 1. (B) The interaction network reconstructed on the training dataset, PBMC 2. (C-D) Each node is colored based on the average expression of the gene in Natural Killer Cells, in (C) training and (D) test datasets (the brighter the higher). (E-F) Each node is colored based on the average importance (based on captum) of the gene in Natural Killer Cells, from (E) MLP and (F) TransformerConv classifiers (the brighter the higher).' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/Gene_Networks.pdf')
``` 

A 2-dimensional UMAP embedding of the training and test datasets after preprocessing is shown in Fig. \@ref(fig:umaps)-A. As one can see, the classes are not clearly separated and such a heterogeneity affects the classification accuracy. 

```{r umaps, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='100%', fig.cap='UMAP embedding. Left: colored by cell types, Right: colored by scRNAseq technology. (A) Raw data with all 260 genes. (B-C) Just using top 2 gene markers per class (14 total) captured by (B) TransformerConv and (C) MLP classifiers.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/umaps.pdf')
``` 

## Classification based on gene interaction networks
We use a recently-introduced graph neural network, called TransformerConv Net [@TC]. In TransformerConv Net, each node generates a message based on its features, and sends it to its neighbors. Then each node aggregates the messages of its neighbors and uses this aggregate to updates its features. The aggregation is done with a permutation-invariant function. In TransformerConv Net, the updated feature vector of node $i$ after one round of message passing is $x_{i}' = W_{1}x_{i} + W_{2}\textrm{mean}_{j\in \mathcal{N}(i)}(x_{j})$ where $W_{1}$ and $W_{2}$ are learnt weight matrices, shared for all of the nodes, and $\mathcal{N}(i)$ is the set of neighbors of $i$. In our setting, each node starts with only one feature (which is the expression level of one gene), but we can have a message-passing function that creates hidden feature vectors of higher dimensions.

As mentioned before, after training the graph neural network, we use captum to find the most important features, based on a well-known interpretation model, Integrated Gradients method [@ig]. We also compared the results to an MLP which does not impose any structure on the input features.  Both MLP and GCN are optimized naively, by selecting a proper learning rate, dropout, and number of hidden layers and hidden nodes. The results are shown in Fig. \@ref(fig:preds). The GCN outperforms the MLP on both training and test datasets. By comparison to the Human Protein Atlas [@hpa], our results suggest how using graphical structure as an inductive bias leads to improvement in interpretation. While, the MLP can not fully capture marker genes of the PBMC cell types, GCN perfectly recovers the characteristic genes, the results are coherent with previous results found by relevant toolkits SCANPY (@scanpy, [notebook](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.htmlm)) and Seurat (@seurat, [notebook](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html)) which essentially use statistical methods to find marker genes. Fig. (fig:umaps)-B&C indicate how precise the GCN could deconvolve the cell types (at least at the broad level), and MLP fails to do so. While, the MLP simply recovers the genes with the higher mean in each class, GCN recovers the true genes or gene modules which are known to contribute in cell identities (Fig. \@ref(fig:nets)-E&F). 


```{r preds, message=FALSE, warning=FALSE, paged.print=FALSE, out.width='100%', fig.cap='The graphical abstract of scGCN.' , echo=F}
knitr::include_graphics('../../output/pbmc/figures/Predictions.pdf')
``` 

# Discussion
We developed new tools for leveraging gene interactions in single-cell data analysis. We showed that how such a structural information could be employed as an inductive bias for better generalization and interpretation. This project was simply a proof of concept but it sheds light on the importance and utility of interaction networks in the field of single-cell and even more, in other biological applications. 

## Gene network reconstruction and transferability 
We reconstructed gene interaction networks, without any prior knowledge, using the graphical lasso. The graphical lasso leads to sparse yet modular solutions on scRNA-seq data, as shown in Fig. \@ref(fig:nets). This is in line with the fact that the true underlying gene regulatory networks are essentially sparse. Moreover, sparsity reduces the estimation noise and ensures the accuracy of the graph representation. Therefore, imposing such an inductive bias in the GCN framework through the graph structure improves the performance through regularization of the parameter space. One can also compare Fig. \@ref(fig:nets)-A and Fig. \@ref(fig:nets)-B, where the gene-interaction networks are reconstructed on two separate PBMC datasets. Higher compatibility will lead to better transferability of the model along environments. We observe that many gene communities from the PBMC 1 are conversed on the PBMC 2 dataset; suggesting that graphical lasso is a reliable method for reconstructing gene networks with scRNA-seq data, in that, it preserves the global network structures. 

## Potential applications
Although here we just focused on gene-gene interaction networks and reconstructed them, the pipeline can be also used on other data modalities (e.g., chromatin structure), where the interaction network is already given. For instance, Hi-C data [@hic] can be used to reconstruct genomic interaction networks, and the rest of the pipeline, that is, classification and interpretation can be used as is, to find marker genomic locations. 

Genomic locations and gene proximity on chromosomes can be used as an additional information for cell annotation. The approach is inspired by the fact that genes in proximity on the chromosomes, are more likely to undergo the same epigenetic events, such as chromatin openness and genomic interactions (e.g., in topologically associating domains, TADs). One can sort genes based on genomic locations. By doing so, each cell can be represented as either a 1-dimensional signal in which each (time) point represents the expression/activation level of the corresponding gene, or a linear graph in which each node is assigned by the expression level of the corresponding gene. By such a modeling, one can use both 1-dimensional Convolutional Neural Networks and Graph Neural Networks. Both of these models are implemented in our pipeline (discussed later), to perform supervised learning.

## Software developed
## Future steps



\newpage
# Acknowledgments {.unnumbered}
Hereby, I want to appreciate extremely helpful comments of Will Macnair, Izaksun Maluma, and Stephany Orjuela. I thank my collaborators in the related previous projects, Laleh Haghverdi and Etienne Sollier. Finally, I express my regards to the STA426 instructors, Mark Robinson, Hubert Rehrauer, and Ahmadreza Yousefkhani for motivating me to conduct this piece of research.


# Code and data availability {.unnumbered}

- Code: [GitHub repository](https://github.com/EliHei2/scGCN) 
- PBMC dataset: [link](https://singlecell.broadinstitute.org/single_cell/study/SCP424/single-cell-comparison-pbmc-data)

<!-- # Appendix {.unnumbered}
## Glossary {.unnumbered}
- **Fluency:** in cognitive sciences and its related fields the concept of 'fluency' refers to 'the subjective experience of ease' with which people undergo a cognitive situation, e.g., processing fluency (ease of processing information), perception fluency (ease of perceptual processing), and retrieval fluencey (ease of retrieving information) [@alter2009uniting].
- **Desirable difficulty:** certain difficulties introduced into the learning process which can greatly improve long-term retention of the learned material [@bjork1994memory].
- **Metacognition:** one's knowledge about their own cognitive processes, or roughly, 'thinking about thinking' [@flavell1976metacognitive].
- **Judgment of learning:** the assessment one makes about how well they have learned and predictions about how well they will remember the learned information [@son2005judgments]. -->
\newpage
# References {-}